{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo_bic.png\" width=\"20%\" align=\"right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Algorithmic Trading\n",
    "## EMH and Algorithmic Trading Quiz\n",
    "\n",
    "&copy; Dr. Yves J. Hilpisch<br>\n",
    "AI-Powered by GPT 5.1<br>\n",
    "The Python Quants GmbH | https://tpq.io<br>\n",
    "https://hilpisch.com | https://linktr.ee/dyjh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "This notebook provides a set of multiple-choice questions (MCQs) to review core ideas from the article and slide deck on **Python for Algorithmic Trading**: Efficient Market Hypothesis (EMH), random walks, backtesting concepts, regression and causality, and streaming.\n",
    "\n",
    "- Each question has **four options (A–D)**.\n",
    "- The **correct answer and explanations** are hidden in a collapsible block.\n",
    "- Click on **“Show answer”** to reveal the solution and reasoning, including why the other options are not appropriate.\n",
    "\n",
    "Use the quiz as a self-check while working through the article, slides, and companion code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 — EMH Basics\n",
    "\n",
    "Which statement best reflects the **Weak-form Efficient Market Hypothesis (EMH)**?\n",
    "\n",
    "A. Prices fully reflect all public and private information.\n",
    "\n",
    "B. Prices fully reflect all **past price and volume information**.\n",
    "\n",
    "C. Prices follow a deterministic trend that can be forecasted from macro data.\n",
    "\n",
    "D. Prices are always equal to fundamental value.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- **A** describes the **Strong-form EMH**, which includes private information.\n",
    "- **B** is the Weak form: past prices and volumes cannot be used to earn abnormal returns.\n",
    "- **C** assumes deterministic predictability from macro data, which is not part of EMH.\n",
    "- **D** is too strong: even in EMH, prices can deviate from fundamentals as long as deviations are not systematically exploitable.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 — Random Walk Interpretation\n",
    "\n",
    "In the article’s random-walk benchmark, daily log-returns $r_t$ are modeled as independent draws from a normal distribution with mean $\\mu$ and volatility $\\sigma$. What does this imply for **tomorrow’s return** $r_{t+1}$ given today’s history $\\{r_1, \\dots, r_t\\}$?\n",
    "\n",
    "A. $r_{t+1}$ is perfectly predictable from $r_t$.\n",
    "\n",
    "B. $r_{t+1}$ has the same distribution as past returns and is **independent** of them.\n",
    "\n",
    "C. $r_{t+1}$ depends only on the last return $r_t$.\n",
    "\n",
    "D. $r_{t+1}$ can be forecasted exactly from the sample mean.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- Under the i.i.d. assumption, $r_{t+1}$ is independent of the history and follows the same distribution.\n",
    "- **A** and **C** incorrectly assume deterministic or autoregressive dependence.\n",
    "- **D** confuses estimating the *mean* with predicting individual realizations; the sample mean does not make $r_{t+1}$ predictable path by path.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 — Autocorrelation and Efficiency\n",
    "\n",
    "The article compares autocorrelation functions (ACFs) for simulated efficient and predictable returns. In an **efficient** market under the random-walk benchmark, what should the sample autocorrelations of daily log-returns look like (apart from estimation noise)?\n",
    "\n",
    "A. Large positive autocorrelations at many lags.\n",
    "\n",
    "B. Large negative autocorrelations at all lags.\n",
    "\n",
    "C. Close to zero at all non-zero lags.\n",
    "\n",
    "D. Exactly $+1$ at lag 1 and zero otherwise.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** C.\n",
    "\n",
    "- For i.i.d. returns, population autocorrelations at non-zero lags are zero; sample estimates fluctuate around zero.\n",
    "- **A** and **B** imply strong predictability (trend or mean reversion).\n",
    "- **D** describes a (pathologically) deterministic process, not a random walk.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 — Testing for Linear Predictability\n",
    "\n",
    "The article outlines a simple AR(1) regression test\n",
    "$$ r_t = \\alpha + \\phi r_{t-1} + \\varepsilon_t $$\n",
    "to check for linear predictability. Which interpretation of the **null hypothesis** is appropriate?\n",
    "\n",
    "A. $H_0$: $\\phi = 0$, returns are not linearly predictable from their own lag.\n",
    "\n",
    "B. $H_0$: $\\phi = 1$, returns follow a random walk.\n",
    "\n",
    "C. $H_0$: $\\phi < 0$, returns are mean-reverting.\n",
    "\n",
    "D. $H_0$: $\\phi > 0$, returns are trending.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** A.\n",
    "\n",
    "- The null states that yesterday’s return contains no linear predictive information about today’s return.\n",
    "- **B** confuses returns with prices; a random walk in prices corresponds to uncorrelated returns, not $\\phi = 1$ in returns.\n",
    "- **C** and **D** are directional alternatives: negative $\\phi$ suggests mean reversion, positive $\\phi$ suggests trend-following.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 — EMH and Trading Strategies\n",
    "\n",
    "If markets were perfectly efficient in the weak form, what would that imply for a trading strategy based **only on past prices** (for example, moving-average crossovers) *before costs*?\n",
    "\n",
    "A. It could still systematically generate abnormal risk-adjusted returns.\n",
    "\n",
    "B. On average it should not generate persistent abnormal risk-adjusted returns.\n",
    "\n",
    "C. It must always lose money in expectation.\n",
    "\n",
    "D. It must have strictly positive Sharpe ratio.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- Under weak-form EMH, strategies based solely on past prices cannot yield **systematic** abnormal returns after proper risk adjustment; any outperformance should be indistinguishable from luck.\n",
    "- **A** contradicts the core implication of weak-form efficiency.\n",
    "- **C** is too strong: efficiency does not require negative expected returns.\n",
    "- **D** is also too strong: a positive Sharpe ratio would indicate a persistent edge.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 — Vectorized Backtesting: Core Idea\n",
    "\n",
    "Which statement best describes **vectorized backtesting** as used in the article?\n",
    "\n",
    "A. It runs the strategy in a compiled C++ engine instead of Python.\n",
    "\n",
    "B. It writes a for-loop over all days to update positions step by step.\n",
    "\n",
    "C. It expresses prices, signals, and positions as arrays and uses array operations instead of explicit Python loops.\n",
    "\n",
    "D. It always simulates limit-order books tick by tick.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** C.\n",
    "\n",
    "- Vectorized backtesting uses NumPy and pandas array operations to implement strategy logic on entire time series at once.\n",
    "- **A** and **B** describe implementation details that are not the defining feature of vectorization.\n",
    "- **D** refers to high-frequency, tick-level simulation, which is beyond the scope of the vectorized daily-bar examples.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 — Lagged-Returns OLS Strategy\n",
    "\n",
    "In the lagged-returns OLS example, we fit\n",
    "$$ r_t = \\alpha + \\beta^\\top x_t + \\varepsilon_t, $$\n",
    "where $x_t$ contains lagged returns $(r_{t-1}, \\dots, r_{t-p})$. Why does this setup avoid **look-ahead bias** when we apply positions based on $\\hat{r}_t$ to the realized $r_t$?\n",
    "\n",
    "A. Because $x_t$ uses only information available up to $t-1$.\n",
    "\n",
    "B. Because we forecast $r_{t+1}$ using $x_{t+1}$.\n",
    "\n",
    "C. Because we shuffle the time index randomly.\n",
    "\n",
    "D. Because we normalise the returns.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** A.\n",
    "\n",
    "- The regressor vector $x_t$ is built from past returns only, so $\\hat{r}_t$ depends on information available by the end of day $t-1$.\n",
    "- **B** would be look-ahead biased: using $x_{t+1}$ requires knowledge of $r_t$ at decision time.\n",
    "- **C** and **D** are data transformations but do not by themselves address information timing.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 — Transaction Costs in Vectorized Backtests\n",
    "\n",
    "The article models transaction costs using **turnover**, defined via changes in the position array. Which of the following captures this idea?\n",
    "\n",
    "A. Multiply strategy returns by a constant spread at every time step.\n",
    "\n",
    "B. Subtract a cost proportional to $|\\text{pos}_t - \\text{pos}_{t-1}|$ from returns whenever positions change.\n",
    "\n",
    "C. Add a random noise term to returns to mimic slippage.\n",
    "\n",
    "D. Ignore costs because they average out over time.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- In the examples, turnover is $|\\text{pos}_t - \\text{pos}_{t-1}|$, and costs are proportional to this quantity.\n",
    "- **A** does not condition on trading activity.\n",
    "- **C** introduces additional randomness but does not tie costs directly to trades.\n",
    "- **D** is unsafe: costs are a major driver of strategy viability and must be modelled explicitly.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 — In-Sample Versus Out-of-Sample\n",
    "\n",
    "The article and notebooks emphasise that the main examples are **in-sample**. What is the main danger of evaluating a strategy **only** on in-sample performance?\n",
    "\n",
    "A. The code will run more slowly.\n",
    "\n",
    "B. The statistical tests become invalid.\n",
    "\n",
    "C. The strategy may be overfitted to historical noise and fail on new data.\n",
    "\n",
    "D. The Sharpe ratio cannot be computed.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** C.\n",
    "\n",
    "- Without out-of-sample or walk-forward validation, a strategy can latch onto noise patterns that do not generalise.\n",
    "- **A** and **D** are unrelated to the in-sample/out-of-sample split.\n",
    "- **B** is too strong: tests can still be computed, but their interpretation with respect to future performance is fragile.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 — Event-Based Backtesting Motivation\n",
    "\n",
    "Which of the following is a **primary motivation** for moving from vectorized to event-based backtesting in the article?\n",
    "\n",
    "A. To speed up NumPy matrix multiplications.\n",
    "\n",
    "B. To model order generation, fills, and portfolio-level constraints more realistically.\n",
    "\n",
    "C. To avoid using pandas DataFrames.\n",
    "\n",
    "D. To guarantee higher Sharpe ratios.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- Event-based engines can capture mechanics such as order queues, fills, and cross-asset constraints that are hard to express in pure array form.\n",
    "- **A** is not the main driver; vectorized code is already efficient for daily-bar data.\n",
    "- **C** is incidental; pandas can still be used in event-based frameworks.\n",
    "- **D** conflates modelling realism with performance; there is no guarantee of higher Sharpe ratios.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11 — Core Components of the Event-Based Framework\n",
    "\n",
    "Which set of components matches the **minimal event-based architecture** described in the article and implemented in the code?\n",
    "\n",
    "A. DataHandler, Strategy, Portfolio, ExecutionHandler.\n",
    "\n",
    "B. Loader, Plotter, Database, Logger.\n",
    "\n",
    "C. APIClient, RiskEngine, OrderRouter.\n",
    "\n",
    "D. SignalGenerator, RiskModel, Broker, Exchange.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** A.\n",
    "\n",
    "- The minimal architecture revolves around a DataHandler, Strategy, Portfolio, and ExecutionHandler exchanging events via a queue.\n",
    "- **B**, **C**, and **D** list plausible components but do not match the specific minimal set used in the article and scripts.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12 — Equity Curves and Drawdowns\n",
    "\n",
    "In the article, equity curves are constructed as cumulative products of $(1 + r_t)$. Which of the following statements about **maximum drawdown** is most accurate in this context?\n",
    "\n",
    "A. Maximum drawdown is the minimum equity level ever reached.\n",
    "\n",
    "B. Maximum drawdown measures the worst percentage loss from a historical peak to a subsequent trough.\n",
    "\n",
    "C. Maximum drawdown is just the sample standard deviation of returns.\n",
    "\n",
    "D. Maximum drawdown is irrelevant when using log-returns.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- In the code, maximum drawdown is computed as the minimum of equity divided by its running maximum minus one.\n",
    "- **A** ignores the reference to the **peak** and would misinterpret the level.\n",
    "- **C** confuses volatility with drawdown.\n",
    "- **D** is incorrect: drawdown is defined on equity, independent of whether we started from simple or log-returns.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13 — Correlation vs. Causation\n",
    "\n",
    "The article stresses the distinction between **correlation and causation**. Which statement is most accurate?\n",
    "\n",
    "A. If $X$ and $Y$ are correlated, $X$ must cause $Y$.\n",
    "\n",
    "B. If $X$ causes $Y$, they are always uncorrelated.\n",
    "\n",
    "C. Causation can lead to correlation, but correlation alone does not establish causation.\n",
    "\n",
    "D. Correlation and causation are the same in time series.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** C.\n",
    "\n",
    "- A causal relationship often implies some form of statistical dependence, but the reverse is not true: correlation can be driven by confounders, common trends, or pure chance.\n",
    "- **A** and **D** overstate what correlation implies.\n",
    "- **B** is incorrect: causal effects usually do show up as correlations unless hidden by noise or measurement issues.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14 — Granger Causality (Concept)\n",
    "\n",
    "In the article’s Granger-causality discussion, series $X$ is said to **Granger-cause** series $Y$ if:\n",
    "\n",
    "A. Changes in $X$ mechanically determine the value of $Y$.\n",
    "\n",
    "B. Knowing the past of $X$ improves forecasts of $Y$ beyond what can be achieved using only the past of $Y$.\n",
    "\n",
    "C. $X$ and $Y$ have zero contemporaneous correlation.\n",
    "\n",
    "D. $X$ and $Y$ always move in opposite directions.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- Granger causality is a **predictive** notion: past $X$ adds explanatory power for $Y$ beyond $Y$’s own history.\n",
    "- **A** would be a much stronger, structural notion of causality.\n",
    "- **C** and **D** describe particular correlation patterns, not the Granger concept.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15 — R-squared in Regression\n",
    "\n",
    "When comparing restricted and full regressions in the Granger examples, the article uses **$R^2$** as a summary. Which interpretation is appropriate?\n",
    "\n",
    "A. $R^2$ measures the proportion of variance in the dependent variable explained by the regressors.\n",
    "\n",
    "B. $R^2$ is the square root of the Sharpe ratio.\n",
    "\n",
    "C. $R^2$ is always equal to the correlation between residuals and fitted values.\n",
    "\n",
    "D. $R^2$ must be zero if there is any noise in the data.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** A.\n",
    "\n",
    "- By definition, $R^2 = 1 - \\text{SS}_{\\text{res}} / \\text{SS}_{\\text{tot}}$, the fraction of total variance captured by the fitted model.\n",
    "- **B** conflates different concepts (risk-adjusted return vs. fit quality).\n",
    "- **C** is incorrect; $R^2$ is not defined that way.\n",
    "- **D** is wrong: many useful regressions have $0 < R^2 < 1$ despite noisy data.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16 — Streaming with ZeroMQ\n",
    "\n",
    "In the streaming section, ZeroMQ is used to decouple tick **producers** and **consumers**. Which pattern best matches the simple examples?\n",
    "\n",
    "A. A `PUSH` socket sending to multiple `PULL` sockets.\n",
    "\n",
    "B. A `REQ` socket sending to a `REP` socket.\n",
    "\n",
    "C. A `PUB` socket broadcasting to one or more `SUB` sockets.\n",
    "\n",
    "D. A `ROUTER` socket connected to a `DEALER` socket.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** C.\n",
    "\n",
    "- The tick server publishes price updates via a `PUB` socket; clients subscribe using `SUB` sockets.\n",
    "- **A**, **B**, and **D** are valid ZeroMQ patterns but are not the ones used in the introductory streaming examples.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 17 — Online Statistics for Streaming Data\n",
    "\n",
    "Why does the article and corresponding code emphasise **online mean and variance estimation** for streaming returns?\n",
    "\n",
    "A. To avoid storing the entire return history in memory.\n",
    "\n",
    "B. Because batch statistics are mathematically invalid.\n",
    "\n",
    "C. To ensure that the Sharpe ratio is always positive.\n",
    "\n",
    "D. To make the code compatible only with NumPy.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** A.\n",
    "\n",
    "- Online algorithms update summary statistics incrementally without keeping all past observations, which is crucial for long or unbounded streams.\n",
    "- **B** is incorrect; batch statistics are fine when storage allows.\n",
    "- **C** and **D** are unrelated to the motivation for online formulas.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 18 — Overfitting in Algorithmic Trading\n",
    "\n",
    "Which of the following is a strong **indicator of overfitting** in a backtest?\n",
    "\n",
    "A. The strategy performs similarly across many choices of hyperparameters.\n",
    "\n",
    "B. Small parameter changes lead to large performance swings, and only a very narrow configuration looks good.\n",
    "\n",
    "C. The strategy underperforms buy-and-hold.\n",
    "\n",
    "D. The code runs slowly on large datasets.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- A \"knife-edge\" strategy that is only profitable for very specific parameters and fails elsewhere is a classic overfitting warning sign.\n",
    "- **A** suggests robustness rather than overfitting.\n",
    "- **C** is disappointing but not necessarily evidence of overfitting.\n",
    "- **D** is merely a performance issue, not a statistical one.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 19 — Sharpe Ratio Interpretation\n",
    "\n",
    "In the performance tables, the article reports **annualised return** and **annualised volatility** to compute a Sharpe ratio (ignoring the risk-free rate for simplicity). Which interpretation is appropriate?\n",
    "\n",
    "A. Sharpe = annualised return divided by annualised volatility.\n",
    "\n",
    "B. Sharpe = total return divided by maximum drawdown.\n",
    "\n",
    "C. Sharpe = correlation between returns and a benchmark.\n",
    "\n",
    "D. Sharpe = 1 / (1 + volatility).\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** A.\n",
    "\n",
    "- The simplified Sharpe ratio is $\\text{Sharpe} = \\mu_{\\text{ann}} / \\sigma_{\\text{ann}}$ when the risk-free rate is set to zero.\n",
    "- **B** resembles a return-over-drawdown metric, not the Sharpe.\n",
    "- **C** refers to correlation, which is a different concept.\n",
    "- **D** is not a recognised risk-adjusted performance measure.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 20 — Curse of Asymmetry\n",
    "\n",
    "The article and slides mention the **“curse of asymmetry”** regarding drawdowns and recoveries. Which example correctly illustrates this idea?\n",
    "\n",
    "A. A 20% loss requires a 20% gain to break even.\n",
    "\n",
    "B. A 50% loss requires a 100% gain to break even.\n",
    "\n",
    "C. A 10% loss requires a 5% gain to break even.\n",
    "\n",
    "D. Any loss can be undone by the same percentage gain.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- If equity drops from 1.0 to 0.5 (a 50% loss), it must double (a 100% gain) to return to 1.0.\n",
    "- For a 20% loss, the required gain is $0.25$ (25%), not 20%.\n",
    "- **C** and **D** are numerically incorrect: percentage gains and losses are not symmetric.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 21 — EMH and Model Complexity\n",
    "\n",
    "Suppose you fit a very complex machine-learning model to returns that are in fact i.i.d. noise. Which outcome is most likely in backtests and out-of-sample?\n",
    "\n",
    "A. The model will find a genuine edge and outperform out-of-sample.\n",
    "\n",
    "B. The model will show strong in-sample performance but fail to generalise out-of-sample.\n",
    "\n",
    "C. The model will perform poorly both in-sample and out-of-sample.\n",
    "\n",
    "D. The model will perfectly predict all future returns.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- A highly flexible model can overfit noise and appear strong in-sample despite there being no true signal; out-of-sample performance then collapses.\n",
    "- **A** and **D** assume a genuine edge in pure noise, which contradicts the setup.\n",
    "- **C** is possible if regularisation is extreme, but the typical overfitting pattern is (B).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 22 — Lopez de Prado’s Perspective on Backtesting\n",
    "\n",
    "The article cites Marcos López de Prado’s view that **backtesting is not the research tool itself**. What is the main point of this argument?\n",
    "\n",
    "A. Backtesting is useless and should be avoided.\n",
    "\n",
    "B. The real research lies in designing robust features and signals; backtests are just a way to check whether these ideas hold up.\n",
    "\n",
    "C. Only deep learning models need backtests.\n",
    "\n",
    "D. Simple technical indicators are always sufficient.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- López de Prado emphasises that discovering meaningful predictors and understanding causal structure is the core research task; backtesting is a diagnostic, not a substitute for genuine insight.\n",
    "- **A** overstates the critique: backtesting remains necessary but not sufficient.\n",
    "- **C** and **D** misrepresent the focus on feature engineering and robust signals.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 23 — Practical Use of EMH in Algorithmic Trading\n",
    "\n",
    "How is the Efficient Market Hypothesis (EMH) **practically useful** for an algorithmic trader, even if markets are not perfectly efficient?\n",
    "\n",
    "A. EMH is purely theoretical and has no practical relevance.\n",
    "\n",
    "B. EMH provides a baseline: strategies should be compared against a random-walk benchmark and efficient scenarios to see whether they add value.\n",
    "\n",
    "C. EMH implies that all strategies must fail, so we should not bother testing them.\n",
    "\n",
    "D. EMH guarantees that any strategy with non-zero Sharpe will work forever.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- EMH-inspired benchmarks (random walks, i.i.d. returns) provide a reference for what “no edge” looks like; strategies should beat such baselines in a statistically robust way.\n",
    "- **A**, **C**, and **D** either dismiss EMH entirely or overinterpret it.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 24 — Data Leakage in Backtesting\n",
    "\n",
    "Which of the following is an example of **data leakage** in a backtest?\n",
    "\n",
    "A. Using yesterday’s close to decide today’s position.\n",
    "\n",
    "B. Normalising features using means and standard deviations computed on the entire dataset, including future observations.\n",
    "\n",
    "C. Subsampling the data to weekly frequency.\n",
    "\n",
    "D. Including transaction costs in the performance metrics.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- Normalising with statistics computed on the full sample allows information from the future to leak into the past.\n",
    "- **A** is fine: yesterday’s close is known at decision time.\n",
    "- **C** is just a resampling choice.\n",
    "- **D** is good practice, not leakage.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 25 — Summary: From EMH to Streaming\n",
    "\n",
    "Which summary best captures the **progression of topics** in the article and companion materials?\n",
    "\n",
    "A. Start with streaming, then regression, then EMH, then backtesting.\n",
    "\n",
    "B. Start with EMH and random walks as a benchmark, then introduce vectorized and event-based backtests, then move to regression, causality diagnostics, and finally streaming architectures.\n",
    "\n",
    "C. Focus only on deep reinforcement learning for trading.\n",
    "\n",
    "D. Skip theory and jump directly to production deployment.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Show answer</strong></summary>\n",
    "\n",
    "**Correct:** B.\n",
    "\n",
    "- The material builds from EMH and random-walk benchmarks, through vectorized and event-based backtesting, into regression-based evaluation, Granger-causality checks, and streaming/ZeroMQ examples.\n",
    "- **A** reverses the intended pedagogical order.\n",
    "- **C** and **D** omit the foundations that make strategies interpretable and robust.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "If you found some of these questions challenging, you may want to revisit:\n",
    "\n",
    "- The **EMH and random-walk** simulations (`1_emh_tests.ipynb`).\n",
    "- The **vectorized lagged-returns strategy** and performance metrics (`2_vec_backtest_ols.ipynb`).\n",
    "- The **event-based momentum example** (`3_event_backtest_momentum.ipynb`).\n",
    "- The **streaming and online statistics** examples (`4_streaming_zeroMQ.ipynb`).\n",
    "\n",
    "The goal is not to memorise answers but to understand how efficient-market benchmarks, statistical tests, backtesting architectures, and streaming infrastructure fit together in a coherent research workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hilpisch.com/tpq_logo_bic.png\" width=\"20%\" align=\"right\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

